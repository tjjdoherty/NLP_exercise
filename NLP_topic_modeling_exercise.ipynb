{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Topic Modeling Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:12.932082Z",
     "start_time": "2020-04-29T12:18:12.200358Z"
    }
   },
   "outputs": [],
   "source": [
    "# import TfidfVectorizer and CountVectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# import fetch_20newsgroups from sklearn.datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# import NMF and LatentDirichletAllocation from sklearn\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:14.463840Z",
     "start_time": "2020-04-29T12:18:13.020189Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create a variable called `'no_features'` and set its value to 100.\n",
    "* create a variable `'no_topics'` and set its value to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:15.590700Z",
     "start_time": "2020-04-29T12:18:15.585820Z"
    }
   },
   "outputs": [],
   "source": [
    "# setting the number of features and topics for the LDA\n",
    "no_features = 100\n",
    "no_topics = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clarity:\n",
    "* The number of features is the number of words the vectorizer will pick up and have as columns for word count (countvectorizer) or topic importance (for tfidf) and gets used in the vectorizer below.\n",
    "\n",
    "* The number of topics is for topic modelling later on with the LDA. **LDA doesnâ€™t directly interact with the vectorizer; it works on the document-term matrix generated by the vectorizer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instantiate a TfidfVectorizer with the following parameters:\n",
    "\n",
    "\n",
    "    * max_df = 0.95\n",
    "    * min_df = 2\n",
    "    * max_features = no_features\n",
    "    * stop_words = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:17.892838Z",
     "start_time": "2020-04-29T12:18:17.888668Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate the TfidfVectorizer with the specified parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95,           # Ignore terms that appear in more than 95% of the documents - avoids common filler and stopwords\n",
    "    min_df=2,              # Ignore terms that appear in less than 2 documents - avoid overfitting and noise\n",
    "    max_features=no_features,  # Limit the number of features (words)\n",
    "    stop_words='english'   # Use English stop words to ignore common words like 'the', 'and', etc.\n",
    ")\n",
    "\n",
    "# vectorizer now ready to transform documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use fit_transform method of TfidfVectorizer to transform the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:21.486038Z",
     "start_time": "2020-04-29T12:18:19.135830Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit transform the documents and store it in the matrix variable for NMF\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* get the features names from TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:22.661253Z",
     "start_time": "2020-04-29T12:18:22.656169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '10' '12' '14' '15' '16' '20' '25' 'a86' 'available' 'ax' 'b8f'\n",
      " 'believe' 'best' 'better' 'bit' 'case' 'com' 'come' 'course' 'data' 'day'\n",
      " 'did' 'didn' 'different' 'does' 'doesn' 'don' 'drive' 'edu' 'fact' 'far'\n",
      " 'file' 'g9v' 'god' 'going' 'good' 'got' 'government' 'help' 'information'\n",
      " 'jesus' 'just' 'key' 'know' 'law' 'let' 'like' 'line' 'list' 'little'\n",
      " 'll' 'long' 'look' 'lot' 'mail' 'make' 'max' 'mr' 'need' 'new' 'number'\n",
      " 'people' 'point' 'power' 'probably' 'problem' 'program' 'question' 'read'\n",
      " 'really' 'right' 'run' 'said' 'say' 'second' 'set' 'software' 'space'\n",
      " 'state' 'sure' 'tell' 'thanks' 'thing' 'things' 'think' 'time' 'true'\n",
      " 'try' 'use' 'used' 'using' 've' 'want' 'way' 'windows' 'work' 'world'\n",
      " 'year' 'years']\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instantiate NMF and fit transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:24.532755Z",
     "start_time": "2020-04-29T12:18:23.661009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [2.34138105e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.15384918e-01]\n",
      " [1.16885597e-08 0.00000000e+00 2.47453911e-02 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  8.05256725e-21 4.92142279e-21]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.18602982e-02 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# as we defined no_topics above\n",
    "no_topics = 100 \n",
    "\n",
    "# Instantiate the NMF model\n",
    "nmf_model = NMF(n_components=no_topics, random_state=42)\n",
    "\n",
    "# Fit the NMF model to the tf-idf matrix and transform it\n",
    "nmf_topic_matrix = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Print the resulting topic matrix (optional)\n",
    "print(nmf_topic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA w/ Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instantiate a CountVectorizer with following parameters:\n",
    "\n",
    "\n",
    "    * max_df = 0.95\n",
    "    * min_df = 2\n",
    "    * max_features = no_features\n",
    "    * stop_words = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:25.547906Z",
     "start_time": "2020-04-29T12:18:25.540452Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate CountVectorizer for LDA\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_df=0.95,           # Ignore terms that appear in more than 95% of the documents - avoids common filler and stopwords\n",
    "    min_df=2,              # Ignore terms that appear in less than 2 documents - avoid overfitting and noise\n",
    "    max_features=no_features,  # Limit the number of features (words)\n",
    "    stop_words='english'   # Use English stop words to ignore common words like 'the', 'and', etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* use fit_transform method of CountVectorizer to transform documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:29.307223Z",
     "start_time": "2020-04-29T12:18:26.637153Z"
    }
   },
   "outputs": [],
   "source": [
    "# make the count matrix\n",
    "\n",
    "count_matrix = count_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* get the features names from TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:18:31.516381Z",
     "start_time": "2020-04-29T12:18:31.498740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '10' '12' '14' '15' '16' '20' '25' 'a86' 'available' 'ax' 'b8f'\n",
      " 'believe' 'best' 'better' 'bit' 'case' 'com' 'come' 'course' 'data' 'day'\n",
      " 'did' 'didn' 'different' 'does' 'doesn' 'don' 'drive' 'edu' 'fact' 'far'\n",
      " 'file' 'g9v' 'god' 'going' 'good' 'got' 'government' 'help' 'information'\n",
      " 'jesus' 'just' 'key' 'know' 'law' 'let' 'like' 'line' 'list' 'little'\n",
      " 'll' 'long' 'look' 'lot' 'mail' 'make' 'max' 'mr' 'need' 'new' 'number'\n",
      " 'people' 'point' 'power' 'probably' 'problem' 'program' 'question' 'read'\n",
      " 'really' 'right' 'run' 'said' 'say' 'second' 'set' 'software' 'space'\n",
      " 'state' 'sure' 'tell' 'thanks' 'thing' 'things' 'think' 'time' 'true'\n",
      " 'try' 'use' 'used' 'using' 've' 'want' 'way' 'windows' 'work' 'world'\n",
      " 'year' 'years']\n"
     ]
    }
   ],
   "source": [
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instantiate LatentDirichletAllocation and fit transformed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:19:03.315322Z",
     "start_time": "2020-04-29T12:18:32.768365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "jesus god know way said really think world read does\n",
      "Topic 1:\n",
      "edu mail like just people max new don time way\n",
      "Topic 2:\n",
      "data use way just like make don better time want\n",
      "Topic 3:\n",
      "jesus people come does make didn course tell look like\n",
      "Topic 4:\n",
      "long just way time like years people know doesn look\n",
      "Topic 5:\n",
      "ax max g9v b8f 25 a86 mr 14 16 good\n",
      "Topic 6:\n",
      "line ll just look like better good way 14 time\n",
      "Topic 7:\n",
      "jesus god say did don 14 point read fact really\n",
      "Topic 8:\n",
      "mr going know don think time ll just day said\n",
      "Topic 9:\n",
      "law fact does people way say think use make come\n",
      "Topic 10:\n",
      "ax b8f a86 g9v max 14 mr 25 10 good\n",
      "Topic 11:\n",
      "state don say better way going time does think good\n",
      "Topic 12:\n",
      "10 20 15 14 25 16 12 00 ll list\n",
      "Topic 13:\n",
      "best better good way probably going just say think doesn\n",
      "Topic 14:\n",
      "key use like using don used way time doesn probably\n",
      "Topic 15:\n",
      "world better 20 information new like just mail used know\n",
      "Topic 16:\n",
      "right just way people like going say don doesn look\n",
      "Topic 17:\n",
      "different like good just ll know don does used point\n",
      "Topic 18:\n",
      "think don just know good probably like look people problem\n",
      "Topic 19:\n",
      "day people way going good better make god think like\n",
      "Topic 20:\n",
      "just going say good way like don think didn point\n",
      "Topic 21:\n",
      "bit 16 better max way just like use does time\n",
      "Topic 22:\n",
      "file windows just use 20 like make thanks help didn\n",
      "Topic 23:\n",
      "little just better good don know say does think like\n",
      "Topic 24:\n",
      "use way does used know don say better like good\n",
      "Topic 25:\n",
      "case way better like think good ll use come thing\n",
      "Topic 26:\n",
      "people just don say government know like make way think\n",
      "Topic 27:\n",
      "00 new don help ve mail going software space work\n",
      "Topic 28:\n",
      "key bit number used information 16 people second using let\n",
      "Topic 29:\n",
      "ax max b8f a86 mr 14 25 ll g9v 12\n",
      "Topic 30:\n",
      "thanks know does windows way just like mail use using\n",
      "Topic 31:\n",
      "don know ll probably going better think just say way\n",
      "Topic 32:\n",
      "space 10 program 14 15 information time new look use\n",
      "Topic 33:\n",
      "doesn ve don just know say ll better point way\n",
      "Topic 34:\n",
      "make better say don way does just use people like\n",
      "Topic 35:\n",
      "power doesn going just like way good don better look\n",
      "Topic 36:\n",
      "need know don ll way going like just doesn does\n",
      "Topic 37:\n",
      "12 16 10 14 15 25 new 20 time way\n",
      "Topic 38:\n",
      "think people better going like don things point good way\n",
      "Topic 39:\n",
      "file program line read year number information long max case\n",
      "Topic 40:\n",
      "true way does people say don know just like doesn\n",
      "Topic 41:\n",
      "drive 16 way just use know max does thanks better\n",
      "Topic 42:\n",
      "try don better just way think good know like make\n",
      "Topic 43:\n",
      "point better use like doesn probably good don time long\n",
      "Topic 44:\n",
      "question way know don probably point make time using doesn\n",
      "Topic 45:\n",
      "want don ll know just going like say look make\n",
      "Topic 46:\n",
      "said people say way time right 15 just fact like\n",
      "Topic 47:\n",
      "new like just ll going time way look know does\n",
      "Topic 48:\n",
      "list space far data time power long new thing let\n",
      "Topic 49:\n",
      "know like don does doesn look just way say question\n",
      "Topic 50:\n",
      "course probably way ll better just doesn know look like\n",
      "Topic 51:\n",
      "way world time people point come say just better think\n",
      "Topic 52:\n",
      "data software available information edu program mail use used 15\n",
      "Topic 53:\n",
      "ll going said say know don let like got come\n",
      "Topic 54:\n",
      "let know don say better just does ll make think\n",
      "Topic 55:\n",
      "ve going work new make long way years say want\n",
      "Topic 56:\n",
      "lot like just better don way know good make right\n",
      "Topic 57:\n",
      "second time just don 15 good know make 10 probably\n",
      "Topic 58:\n",
      "available use probably know like don does used just going\n",
      "Topic 59:\n",
      "did way know say just like don does make point\n",
      "Topic 60:\n",
      "good 25 ll like just look way make know new\n",
      "Topic 61:\n",
      "thing way like good just going know don think really\n",
      "Topic 62:\n",
      "sure just way like make ll know look don come\n",
      "Topic 63:\n",
      "read doesn just know time don does better people think\n",
      "Topic 64:\n",
      "come people think going just say know like don way\n",
      "Topic 65:\n",
      "windows use ll better don doesn just software using program\n",
      "Topic 66:\n",
      "got just going ve way like know good better ll\n",
      "Topic 67:\n",
      "didn know don ll time say like just way people\n",
      "Topic 68:\n",
      "time ll going know good like say better people things\n",
      "Topic 69:\n",
      "like just going don doesn better ll 14 look way\n",
      "Topic 70:\n",
      "set way ll use like time just work know say\n",
      "Topic 71:\n",
      "fact way say doesn time like going point just better\n",
      "Topic 72:\n",
      "years time like say people new just make know probably\n",
      "Topic 73:\n",
      "year years going ll just think time 20 good 10\n",
      "Topic 74:\n",
      "program know like way better does need time help don\n",
      "Topic 75:\n",
      "run going like time just use does ll don 10\n",
      "Topic 76:\n",
      "mail list ll like know going don does use thanks\n",
      "Topic 77:\n",
      "look just like good know going ll want make time\n",
      "Topic 78:\n",
      "work doesn just don way know does better like say\n",
      "Topic 79:\n",
      "file ll like want right know new things problem information\n",
      "Topic 80:\n",
      "far know going say way point course power good just\n",
      "Topic 81:\n",
      "help does know thanks like don use just need better\n",
      "Topic 82:\n",
      "com edu don list just people way 14 like best\n",
      "Topic 83:\n",
      "number 10 use 20 doesn don years 15 time using\n",
      "Topic 84:\n",
      "ve just going way time ll think know look like\n",
      "Topic 85:\n",
      "say believe good think like just don people make better\n",
      "Topic 86:\n",
      "government fact use new used does law going like using\n",
      "Topic 87:\n",
      "things don know way just like going say people look\n",
      "Topic 88:\n",
      "available use edu program information using software file set run\n",
      "Topic 89:\n",
      "information mail know people use way does make new used\n",
      "Topic 90:\n",
      "really doesn going don just like know say does make\n",
      "Topic 91:\n",
      "believe don people know going just point doesn way think\n",
      "Topic 92:\n",
      "god does say people know believe way time good just\n",
      "Topic 93:\n",
      "software does know like don thanks thing good use just\n",
      "Topic 94:\n",
      "using way does use just doesn 10 good don better\n",
      "Topic 95:\n",
      "tell just don better say way think come look does\n",
      "Topic 96:\n",
      "does just know ll better say like make doesn people\n",
      "Topic 97:\n",
      "better year good don think like way doesn ll 20\n",
      "Topic 98:\n",
      "used way better use don say going just good does\n",
      "Topic 99:\n",
      "problem know just going time don way help ve say\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Define number of topics\n",
    "n_topics = 100  # Replace with your predefined number of topics\n",
    "\n",
    "# Instantiate the LDA model\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "\n",
    "# Step 4: Fit the LDA model to the word count matrix\n",
    "lda_topic_matrix = lda_model.fit_transform(count_matrix)\n",
    "\n",
    "# Step 5: Get the feature names (words) from the CountVectorizer\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Step 6: Print the top words for each topic\n",
    "for topic_idx, topic in enumerate(lda_model.components_): # enumerate is used to loop when you need both the index (topix_idx) and the value (topic)\n",
    "    print(f\"Topic {topic_idx}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-11:-1]]))  # Top 10 words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function `display_topics` that is able to display the top words in a topic for different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "Both LDA and NMF models have a `components_` attribute, which contains the topic-word distribution (i.e., the importance of each word for each topic).\n",
    "\n",
    "The `CountVectorizer` (for LDA) and `TfidfVectorizer` (for NMF) provide the feature names (words) using the `get_feature_names_out()` method.\n",
    "\n",
    "Vectorizer creates the vocabulary and transforms the text into a matrix (TF-IDF or raw counts).\n",
    "\n",
    "LDA/NMF works on that matrix to find topics but doesn't store the word names (just word distributions).\n",
    "\n",
    "You need to explicitly pass the feature_names (from the vectorizer) to the function so that the indices of words in the model components can be mapped back to the actual terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:19:04.476192Z",
     "start_time": "2020-04-29T12:19:04.469045Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_): # The trained LDA or NMF model. Both have a components_ attribute that stores the topic-word distributions.\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "\n",
    "        # Get the indices of the top words for each topic\n",
    "        top_word_indices = topic.argsort()[:-no_top_words - 1:-1] # argsort is in descending order by default, here we start at the end of the array which is highest value and go backwards (the last :-1)\n",
    "\n",
    "        # Display the top words for each topic\n",
    "        top_words = [feature_names[i] for i in top_word_indices] # This retrieves the actual word (instead of its index) from the feature names (terms) in the vectorizer.\n",
    "        print(\" \".join(top_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining the above function:\n",
    "\n",
    "* for topic_idx, topic in enumerate because we want both the index and the topic weighting from the model. THe model doesn't store the word name, only the distributions\n",
    "\n",
    "* argsort returns the **index of the weights** in this case and it is **descending order by default**, so `topic.argsort()[:-no_top_words - 1:-1]` will get all the indices corresponding to weights of certain weights of words, and start counting from the very back (the indices of the **largest** weightings), stepping backwards (the last :-1) and end at the smallest weights\n",
    "\n",
    "* top_names gives the **names of the words which comes from the the vectorizer not the model!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* display top 10 words from each topic from NMF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:19:05.672461Z",
     "start_time": "2020-04-29T12:19:05.656545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "don think know want say really try better lot need\n",
      "Topic 1:\n",
      "use windows using problem used file program software drive need\n",
      "Topic 2:\n",
      "does know let doesn help work need want say question\n",
      "Topic 3:\n",
      "edu mail com available information need best list 20 new\n",
      "Topic 4:\n",
      "just right ve way work got say doesn little ll\n",
      "Topic 5:\n",
      "like look things make doesn lot sure really thing ll\n",
      "Topic 6:\n",
      "god believe jesus say true question fact world things way\n",
      "Topic 7:\n",
      "people government right law state said world point fact say\n",
      "Topic 8:\n",
      "thanks mail help information need com tell drive list software\n",
      "Topic 9:\n",
      "good time new year years did ve got make way\n"
     ]
    }
   ],
   "source": [
    "# For NMF model\n",
    "nmf_model = NMF(n_components=10, random_state=42)\n",
    "nmf_topic_matrix = nmf_model.fit_transform(tfidf_matrix)\n",
    "feature_names_nmf = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "display_topics(nmf_model, feature_names_nmf) # default 10 top words but you can use more or less, specify as 3rd argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* display top 10 words from each topic from LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T12:19:06.842806Z",
     "start_time": "2020-04-29T12:19:06.831721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "god jesus believe people say does things know just think\n",
      "Topic 1:\n",
      "edu com 00 new mail list available information best drive\n",
      "Topic 2:\n",
      "use windows key data available using software used information bit\n",
      "Topic 3:\n",
      "space thanks drive problem does program know help work need\n",
      "Topic 4:\n",
      "people government year said years right world new did make\n",
      "Topic 5:\n",
      "ax max g9v b8f a86 14 mr 25 ll 12\n",
      "Topic 6:\n",
      "file power line information program second read case use number\n",
      "Topic 7:\n",
      "10 15 20 16 12 25 14 00 new year\n",
      "Topic 8:\n",
      "don just like think know good ve going time ll\n",
      "Topic 9:\n",
      "law question time true bit used fact day point does\n"
     ]
    }
   ],
   "source": [
    "# example usage of the above function\n",
    "\n",
    "# For LDA model\n",
    "lda_model = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda_topic_matrix = lda_model.fit_transform(count_matrix)\n",
    "feature_names_lda = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "display_topics(lda_model, feature_names_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see some common topics e.g. NMF topic 6 (god, believe, jesus) and LDA topic 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stretch: Use LDA w/ Gensim to do the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be completed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
